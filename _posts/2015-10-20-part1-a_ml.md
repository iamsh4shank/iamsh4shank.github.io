---
layout: post
title: 'Introduction to Adversarial ML'
date: 2023-04-25 11:12:00-0400
description: 'Get started with Security and Privacy in AI'
categories: learnings
tags: machine-learning research
disqus_comments: false
related_posts: true
enabled: true
---

Hi all, let‚Äôs see an interesting case, picture this: you've trained a machine learning model to identify images of dogs and cats accurately. It's performing great during testing and you're excited to deploy it in the real world. But what happens when someone tries to trick the model by feeding it images that are slightly altered, making it misclassify them?

As AI-assisted tools become more prevalent and we witness the upcoming deployment of AI in real-world use cases, it's crucial that we shift our focus not only on several performance metrics but also on the robustness of such models. One thing I have noticed is that we tend to focus more on increasing accuracy or other metrics when creating ML models. For instance, if you're an ML enthusiast who has created models for classification or segmentation tasks, you're likely to evaluate the model's performance based on its accuracy or F1 score. However, we need to consider more factors to ensure that the model doesn't fail in real-world use cases. 

Think about it - what if a camera installed at a busy intersection failed to count the number of cars crossing the road due to external disturbances like weather or background noise? Or worse, what if someone could easily trick a face recognition system with a patch that looks similar to your face? These scenarios highlight the pressing need to make AI systems more secure and resilient to such corruption. These corruptions could be Adversarial attacks, in which an attacker intentionally manipulates data to mislead or evade AI systems, which are one example of the potential risks associated with AI. Additionally, AI systems can potentially reveal sensitive personal information if they are not properly secured.

As I delved deeper into the realm of AI security and privacy, I was surprised to find very less resources to read more on this topic. That's why I've decided to start a conversation on Adversarial ML - a specific area of privacy and security that deals with protecting AI systems from adversarial attacks. Let's dive in!

I would suggest before diving deeper into Adversarial ML, you should have a certain level of familiarity with fundamental deep learning concepts such as optimization, gradient descent, deep networks, and so on. In case you want some good resources you can check out [Deep Learning Book](https://www.deeplearningbook.org/), [PyTorch Tutorials](https://pytorch.org/tutorials/), or any resources that might be helpful to understand the underlying concepts.

So let‚Äôs start but before that why not talk about a few terminologies that we are going to use throughout the series? 

## What are adversarial attacks - 
An adversarial attack is a technique used to intentionally manipulate or trick a machine learning model by introducing carefully crafted input called perturbations that are designed to cause the model to produce incorrect output or in general misclassify the labels. Adversarial attacks can take many forms, but some common examples include adding imperceptible noise to an image or text input, modifying a small subset of the input data, or changing the distribution of the input data. Adversarial attacks are a concern in many machine learning applications, such as computer vision, natural language processing, or speech models.

**Difference between Targeted and Untargeted attack:**
In the case of a non-targeted attack, the generated adversarial images can be assigned to any class but on the other hand, targeted adversarial images are specific to particular classes.

**Difference between White box and Black Box attack:**
A white box attack is an attack scenario where the attacker has access to the target model such as its model‚Äôs architecture and its parameters. On the other hand, a black box attack is an attack scenario where the user has no access to the model but can only access inputs and observe the outputs.

There are two other important methods of attack such as backdoor attack (poisoning) and Gradient-based attack. In the first case, the attacker has access to the network learning process and the training data, and in the latter one, the attacker tries tricking an already trained model by generating adversarial examples from the inputs.

So in this blog let‚Äôs first discuss the gradient-based attack.

<div class="text-center">
    {% include figure.html path="assets/img/adversarial_ml/p1/gd_based_attack.png" class="img-fluid rounded z-depth-1" %}
</div>

Let's begin by exploring some practical applications while simultaneously learning the relevant theory. I will be incorporating some mathematical concepts along the way, which may significantly enhance the complexity of the topic. However, I will try to provide relevant links and explanations to ensure that it remains clear and understandable.

To start, I will use a pre-trained ResNet18 model by PyTorch to classify the image of a Goldfish

<div class="text-center">
    {% include figure.html path="assets/img/adversarial_ml/p1/goldfish.png" class="img-fluid rounded z-depth-1" %}
</div>

```python
from PIL import Image
from torchvision import transforms

gold_fish_img = Image.open("goldfish.png")
transform = transforms.Compose([
   transforms.Resize(224),
   transforms.ToTensor(),
])
gold_fish_tensor = transform(gold_fish_img)[None,:,:,:]
plt.imshow(gold_fish_tensor[0].numpy().transpose(1,2,0))
```
<div class="text-center">
    {% include figure.html path="assets/img/adversarial_ml/p1/load_gd_img.png" class="img-fluid rounded z-depth-1" %}
</div>

Now we have the image next we need a model, so let‚Äôs load the model, in my case I am using a pre-trained ResNet18 model by PyTorch. You can also train your own model based on your use case.

```python
import torchvision.models as models
model = models.resnet18(pretrained = True)
model.eval()
```
And for prediction - 

```python
#Applying normaliztion
trans = transforms.Compose([
   transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  
])
output = model(trans(gold_fish_tensor))
pred = output.argmax(dim = 1, keepdim = True)
```

To be sure let's confrim the predicted label first - 
```python
import json
    with open("/content/imagenet-simple-labels.json") as f:
        imagenet_classes = json.load(f)
    print(imagenet_classes[pred.item()])

```
Ok everything is working fine our model predicted correctly that it is a Goldfish. Now next we need to generate the perturb data. So before that let‚Äôs come to some general concepts of model. So we have input space ***X*** (say), and the output space will be a *k* dimensional vector space (*k* being the number of classes), here the output space vector could be the logit space. So we can define the function for the model something like - 

$$ h_\theta : X \rightarrow \mathbb{R}^k $$

Here ùúÉ represents parameters that we typically optimize when we train a neural network. So finally we can say hùúÉ corresponds to the model. 

Now the other important thing that we optimize is the loss during the training. Now as we all know to calculate loss we need the predicitons and true labels or the ground truth. So we can define our loss as - 

$$ l(h_\theta(x), y) $$

here x ‚àà X and y ‚àà Z, where Z is nothing but the true class. In short we can also say
$$ l: \mathbb{R}^k \times \mathbb{Z}_+ \rightarrow \mathbb{R}_+ $$

it means, loss it nothing but a mapping from the model predictions and the ground truth to a non-negative number (loss value). There are several loss funciton we can use such as log loss, hinge loss, MSE, etc. Let's understand in terms of cross entropy loss. Thw whole idea of training a network is to maximize the probability od the actual class out of all the other classes. We all know as we apply a softmax activation to get the probability distribution which is defined as - 

$$ \sigma(z)_i = \frac{exp(z_i)}{\sum_{j=1}^{k} exp(z_j)}  $$

Also the probabilities are too small so we focus more on log of the probability of the true class which is - 

$$ \log\sigma(h_\theta(x))_y = \log(\frac{exp(h_\theta(x)_y)}{\sum_{j=1}^{k} exp(h_\theta(x)_j)}) = h_\theta(x)_y - \log(\sum_{j=1}^{k} exp(h_\theta(x)_j)) $$

And then rather than maximizing the probability distribution we can minimize the loss which is nothing but the negation of the above equation which is - 

$$ l(hŒ∏(x),y)=log(\sum_{n=1}^{\infty} exp(h_\theta(x)_j)) - h_\theta(x)_y $$

too much maths but for our use case we can simple use this function by PyTorch to use the cross entropy loss - 
```python
# 1 is the label index of Gold Fish
import math
loss = nn.CrossEntropyLoss()(output,torch.LongTensor([1])).item()
probability = math.exp(-loss)
print (f"Loss: {loss}, probability of class Goldfish: {probability}")
```

```
Output: 
Loss: 0.0010887415846809745, probability of class Goldfish: 0.9989118508794051
```
So if apply exp of this we an approximately as probability which is a good accuracy.

Now let's learn how to create the adversarial example

## Generating an adversarial example
Our main aim is to manipulate the input image which will make the model to misclassify the image or believe it is something else. So let's understand why we want to craft perturbation in the input data. As we know for good training we need to minimize the loss such that the model performs well on inputs which we achieve by gradient descent for some minibatch $\beta$ where we compute the gradient of our loss w.r.t. parameters ùúÉ i.e. 

 $$ \theta:= \theta - \frac{\alpha}{\beta}\sum_{i‚àà\beta}\Delta_\theta l(h_\theta(x_i),y_i) $$

Here the interesting part is the gradient ($\Delta_\theta l(h_\theta(x_i),y_i)$), it compuates how the small change in $\theta$ will affect the loss and overall probability. This parameter update is done using the efficient backpropagation algorithm. Here in this we can also compute the effect on loss due to input change i.e. change in loss w.r.t. input $x_i$. So we can make small changes in the input which will overall affect the loss and hence the prediction. So in case of training we update the parameters in such a way so that we minimize the loss but in our case to generate the adversarial example we'll maximize that loss which makes our optimization problem as - 

$$ \underset{\hat{x}}{maximize} \text{ }l(h_\theta(\hat{x}), y) $$

here $\hat{x}$ denotes the adversarial example which is trying to maximize the loss. Now comes the important point, what if we replace this image with some other image let say tigershark (in our case). The model will then misclassify the label. In this case we didn't fool the classifier. So we need to make sure that $\hat{x}$ is close to our original inout $x$. So by convention we can say we can add a specific amount of perturbation in the sample let say $x + \delta$ and this $\delta$ can vary through $\Delta$. So now our optimisation equation is going to be - 

$$ \underset{\delta \in \Delta}{maximize} \text{ }l(h_\theta(x + \delta), y) $$

Now comes the other important topic of discussion i.e. how to decide for this $\Delta$ i.e. allowed set of perturbation. In simple langauge this value can be anything which when added to the actual image, human visually feel no difference with the original input image. So we can apply several types of transformations in the image keeping in mind that it should not affect the semantic content of the image under the added perturbation. Mathematically we define a perturbation set which can be L norms such that we need to bound the added perturbation inside the L norms (\$ L_0, L_1, L_2, or L_\infty \$). The most common is $L_\infty$ ball.

